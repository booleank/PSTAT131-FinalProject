---
title: "PSTAT 131/231 Final Project"
author: "gang"
date: "6/7/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, 
                      fig.align = 'center',
                      fig.height = 4, 
                      fig.width = 4)

library(pander)
library(tidyverse)
library(ggmap)
library(modelr)
library(class)
library(splines)
library(tree)
library(maptree)
library(gbm)
library(ROCR)
setwd("~/Documents/Spring2021/PSTAT131/final-project")
```

```{r echo = F}
# load data
load('merged_data2.RData')
```

## Data Preparation
```{r}
# filter out losing candidates by county => 1 row per county, 'candidate' is winner of the county
county_win_data <- merged_data2 %>%
  group_by(fips) %>%
  slice_max(votes) %>%
  ungroup()

#center and scale
x_mx <- county_win_data %>%
  select(-c('county':'pct')) %>% 
  scale(center = T, scale = T) #centers and scales data

# set rng
set.seed(20121)

# partition data
county_part <- resample_partition(as_tibble(x_mx), c(test = .3, train = .7))
test <- county_part$test
train <- county_part$train
testdata <- as_tibble(x_mx) %>% slice(test$idx) #test dataframe
traindata <- as_tibble(x_mx) %>% slice(train$idx) #training dataframe
```

## PC Analysis

```{r, echo = F, fig.width = 7, fig.height = 7}
# compute loadings for PC1 and PC2
x_svd <- svd(traindata)
d_sq <- x_svd$d^2/(nrow(traindata) - 1)
loadings <- x_svd$v

# Loadings plot
loadings[, 1:10] %>%
  as.data.frame() %>%
  rename(PC1 = V1, PC2 = V2, PC3 = V3, PC4 = V4, PC5 = V5, 
         PC6 = V6, PC7 = V7, PC8 = V8, PC9 = V9, PC10 = V10) %>%
  mutate(variable = colnames(traindata)) %>%
  gather(key = 'PC', value = 'Loading', 1:10) %>%
  arrange(variable) %>%
  ggplot(aes(x = Loading, y = variable)) +
  geom_point(aes(color = PC), size = 0.5) +
  facet_wrap(~ PC) +
  theme_bw(base_size = 12) +
  geom_vline(xintercept = 0, color = 'blue') +
  geom_path(aes(group = PC, color = PC)) +
  labs(y = '')
```


## Scree and Cumulative Variance Plot
Using a scree and cumulative variance plot to observe the correct amount of PC's to use in the regression models.
```{r echo = F, fig.width = 5}
## scree and cumulative variance plots
tibble(PC = 1:min(dim(traindata)),
       Proportion = d_sq/sum(d_sq),
       Cumulative = cumsum(Proportion)) %>%
  gather(key = 'measure', value = 'Variance Explained', 2:3) %>%
  ggplot(aes(x = PC, y = `Variance Explained`)) +
  geom_point(size = 0.75) +
  geom_path() +
  facet_wrap(~ measure) +
  theme_bw(base_size = 6) +
  scale_x_continuous(breaks = 1:31, labels = as.character(1:31)) +
  geom_hline(yintercept = 0.778,
             color = 'red',
             linetype = 2)
sum(d_sq[1:8])/sum(d_sq)
```
To capture 78% of the total variation, we need to use the first 8 PC's.

```{r}
# get training PC data for first 8 PCs
v_q <- loadings[, 1:8] 
Z_train <- as.matrix(traindata) %*% v_q #training PC's
Z_test <- as.matrix(testdata) %*% v_q #test PC's, *calculated on training loadings*

#construct testing and training dataframes
colnames(v_q) <- colnames(Z_train) <- colnames(Z_test) <- paste('PC', 1:8, sep = '')
pc_train <- tibble(winner = factor(county_win_data$candidate[train$idx])) %>%
  bind_cols(as_data_frame(Z_train)) 
pc_train$winner <- fct_collapse(pc_train$winner,
                               Don = 'Donald Trump',
                               Hill = 'Hillary Clinton')

pc_test <- tibble(winner = factor(county_win_data$candidate[test$idx])) %>%
  bind_cols(as_data_frame(Z_test)) 
pc_test$winner <- fct_collapse(pc_test$winner,
                               Don = 'Donald Trump',
                               Hill = 'Hillary Clinton')
```

## Logistic Regression
```{r }
# Logistic regression model on training data using first 8 PCs
glm_model <- glm(winner ~ ., family = 'binomial', data = pc_train)

# compute estimated probabilities
p_hat_glm <- predict(glm_model, pc_test, type = 'response')

# bayes classifier
y_hat_glm <- factor(p_hat_glm > 0.5, labels = c('Don', 'Hill'))

# errors
error_glm <- table(y = pc_test$winner, y_hat_glm)
error_glm

# total misclassification rate
tot_misclass_glm <- 1 - sum(diag(error_glm))/nrow(pc_test)
tot_misclass_glm

# roc curve for glm
prediction_glm <- prediction(predictions = p_hat_glm, labels = pc_test$winner)

# compute error rates as a function of probability threshhold
perf_glm <- performance(prediction.obj = prediction_glm, 'tpr', 'fpr')

# extract error rates as a tibble
rates_glm <- tibble(fpr = slot(perf_glm, 'x.values'),
                  tpr = slot(perf_glm, 'y.values'),
                  thresh = slot(perf_glm, 'alpha.values')) %>%
  unnest(everything()) %>%
  mutate(method = 'glm',
         youden = tpr - fpr)

# plot roc curve and optimal threshold
rates_glm %>%
  ggplot(aes(x = fpr, y = tpr)) +
  geom_path(aes(color = thresh), size = 1) +
  scale_color_binned(type = 'viridis') +
  guides(color = guide_bins()) +
  theme_bw() +
  geom_point(data = slice_max(rates_glm, youden),
             shape = 16, color = 'red')

# store optimal threshold
optimal_thresh <- slice_max(rates_glm, youden)

# recalibrate qda with different probability threshold
preds_glm_adj <- factor(p_hat_glm > optimal_thresh$thresh,
                        labels = c(0, 1))

# cross-tabulate estimated and true classes with adjusted threshold
errors_glm_adj <- table(class = pc_test$winner, pred = preds_glm_adj)
errors_glm_adj

misclass_glm_adj = 1-sum(diag(errors_glm_adj))/nrow(pc_test)
misclass_glm_adj
```

## K Nearest Neighbors
```{r}
y <- (pc_train %>% pull(winner))

# leave one out cross validation
cv_out <- tibble(k = seq_range(5:50, n = 20, pretty = T)) %>%
  mutate(loocv_preds = map(k, ~ knn.cv(pc_train[-1], y, .x)),
         class = map(k, ~ y)) %>%
  mutate(misclass = map2(loocv_preds, class, 
                         ~ as.numeric(.x) - as.numeric(.y))) %>%
  mutate(error = map(misclass, ~ mean(abs(.x))))

# error rates for each k
cv_errors <- cv_out %>% 
  select(k, error) %>% 
  unnest(everything()) 

# plot errors against k
cv_errors %>%
  ggplot() +
  geom_line(aes(x = k, y = error), color = 'cornflowerblue') +
  theme_bw()

# select k
best_k <- cv_errors$k[which.min(cv_errors$error)]

# re-train with best k
pred_knn <- knn(train = pc_train[-1], test = pc_test[-1], cl = y, k = best_k)

# misclassifications
error_knn <- table(pc_test$winner, pred_knn)
error_knn

# total misclassification rate
tot_misclass_knn <- 1 - sum(diag(error_knn))/nrow(pc_test)
tot_misclass_knn
```

So far KNN performed better than logistic regression, based on the total misclassification error rate metric.

## Regression Tree
```{r}
#initial tree
nmin <- 5
tree_opts <- tree.control(nobs = nrow(pc_train), 
                          minsize = nmin, 
                          mindev = 0)
t_0 <- tree(winner ~ ., data = pc_train,
                control = tree_opts, split = 'deviance') 

nfolds <- 25
cv_out <- cv.tree(t_0, K = nfolds)
cv_df <- tibble(alpha = cv_out$k,
                impurity = cv_out$dev,
                size = cv_out$size)
best_alpha <- slice_min(cv_df, impurity) %>%
  slice_min(size)

# prune initial tree
t_opt <- prune.tree(t_0, k = best_alpha$alpha)

# plot
draw.tree(t_opt, cex = 0.8, size = 2, digits = 1)

# class probabilities on test partition
probs <- predict(t_opt, pc_test)

# predicted class labels
preds <- factor(probs[, 2] > 0.5, labels = c('Don', 'Hill'))

# misclassification error rates
error_tree <- table(pred = preds, class = pc_test$winner)
misclass_tree = 1-sum(diag(error_tree))/nrow(pc_test)
misclass_tree
```

## Random Forest 
```{R}
pc_train_rf <- pc_train %>%
  mutate(winner = ifelse(winner == "Hill",1,0))
pc_test_rf <- pc_test %>%
      mutate(winner = ifelse(winner == "Hill",1,0))



fit_gbm <- gbm(winner ~ ., data = pc_train_rf, distribution ='adaboost', interaction.depth =3, n.trees =100, cv.folds = 5)

# select boosting iterations
best_m <- gbm.perf(fit_gbm, method ='cv')

# compute predictions on test set
preds <- predict(fit_gbm, pc_test_rf, n.trees = best_m)
probs <-1/(1+ exp(-preds))
y_hat <- factor(probs >0.5, labels = c('Don','Hill'))
y <- factor(pc_test_rf$winner, labels = c('Don','Hill'))



# compute misclassification errors
rf_errors <- table(class = y, pred = y_hat)
misclass_rf = 1-sum(diag(rf_errors))/nrow(pc_test)
misclass_rf

rf_errors
```
#Random forest shows lowest total misclass.

## Linear Discriminant Analysis (LDA):
```{r}
# fit lda model
fit_lda <- MASS::lda(winner ~ ., method = 'mle', data = pc_train_rf)

# compute estimated classes
preds_lda <- predict(fit_lda, pc_train_rf)

# cross-tabulate estimated and true classes
errors_lda <- table(class = pc_train_rf$winner, pred = preds_lda$class)
misclass_lda = 1-sum(diag(errors_lda))/nrow(pc_train)
misclass_lda

# roc curve for lda
prediction_lda <- prediction(predictions = preds_lda$posterior[, 2], labels = pc_train_rf$winner)

# compute error rates as a function of probability threshhold
perf_lda <- performance(prediction.obj = prediction_lda, 'tpr', 'fpr')

# extract error rates as a tibble
rates_lda <- tibble(fpr = slot(perf_lda, 'x.values'),
                  tpr = slot(perf_lda, 'y.values'),
                  thresh = slot(perf_lda, 'alpha.values')) %>%
  unnest(everything()) %>%
  mutate(method = 'lda',
         youden = tpr - fpr)

# plot roc curve and optimal threshold
rates_lda %>%
  ggplot(aes(x = fpr, y = tpr)) +
  geom_path(aes(color = thresh), size = 1) +
  scale_color_binned(type = 'viridis') +
  guides(color = guide_bins()) +
  theme_bw() +
  geom_point(data = slice_max(rates_lda, youden),
             shape = 16, color = 'red')

# store optimal threshold
optimal_thresh <- slice_max(rates_lda, youden)

# recalibrate qda with different probability threshold
preds_lda_adj <- factor(preds_lda$posterior[, 2] > optimal_thresh$thresh,
                        labels = c(0, 1))

# cross-tabulate estimated and true classes with adjusted threshold
errors_lda_adj <- table(class = pc_train_rf$winner, pred = preds_lda_adj)

misclass_lda_adj = 1-sum(diag(errors_lda_adj))/nrow(pc_train)
misclass_lda_adj
```

## Quadratic Discriminant Analysis (QDA):
```{r}
# fit qda model
fit_qda <- MASS::qda(winner ~ ., method = 'mle', data = pc_train_rf)

# compute estimated classes
preds_qda <- predict(fit_qda, pc_train_rf)

# cross-tabulate estimated and true classes
errors_qda <- table(class = pc_train_rf$winner, pred = preds_qda$class)
misclass_qda = 1-sum(diag(errors_qda))/nrow(pc_train)
misclass_qda

# roc curve for qda
prediction_qda <- prediction(predictions = preds_qda$posterior[, 2], labels = pc_train_rf$winner)

# compute error rates as a function of probability threshhold
perf_qda <- performance(prediction.obj = prediction_qda, 'tpr', 'fpr')

# extract error rates as a tibble
rates_qda <- tibble(fpr = slot(perf_qda, 'x.values'),
                  tpr = slot(perf_qda, 'y.values'),
                  thresh = slot(perf_qda, 'alpha.values')) %>%
  unnest(everything()) %>%
  mutate(method = 'qda',
         youden = tpr - fpr)

# plot roc curve and optimal threshold
rates_qda %>%
  ggplot(aes(x = fpr, y = tpr)) +
  geom_path(aes(color = thresh), size = 1) +
  scale_color_binned(type = 'viridis') +
  guides(color = guide_bins()) +
  theme_bw() +
  geom_point(data = slice_max(rates_qda, youden),
             shape = 16, color = 'red')

# store optimal threshold
optimal_thresh <- slice_max(rates_qda, youden)

# recalibrate qda with different probability threshold
preds_qda_adj <- factor(preds_qda$posterior[, 2] > optimal_thresh$thresh,
                        labels = c(1, 0))

# cross-tabulate estimated and true classes with adjusted threshold
errors_qda_adj <- table(class = pc_train_rf$winner, pred = preds_qda_adj)
misclass_qda_adj = 1-sum(diag(errors_qda_adj))/nrow(pc_train)
misclass_qda_adj
```

# Misclassification Rates Table
```{r}
misclass.rates <- data.frame("Regression Model" = c("K Nearest Neighbors", "Random Forest", "Logistic Regression", "Random Tree", "Quadratic Discriminant Analysis", "Linear Discriminant Analysis"), "Misclassification Rate" = c(tot_misclass_knn, misclass_rf, tot_misclass_glm, misclass_tree, misclass_qda_adj, misclass_lda_adj))

#write.csv(misclass.rates, 'missclass_rates.csv', row.names = FALSE)

misclass.rates %>% pander()
```
