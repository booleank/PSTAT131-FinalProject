---
title: "PSTAT 131/231 Final Project"
author: "gang"
date: "6/7/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, 
                      fig.align = 'center',
                      fig.height = 4, 
                      fig.width = 4)

library(pander)
library(tidyverse)
library(ggmap)
library(modelr)
library(class)
library(splines)
library(tree)
library(maptree)
library(gbm)
library(ROCR)
setwd("~/Documents/Spring2021/PSTAT131/final-project")
```

```{r echo = F}
# load data
load('merged_data2.RData')
```

## PC Analysis

```{r, echo = F, fig.width = 7, fig.height = 7}
# center and scale
county_win_data <- merged_data2 %>%
  group_by(fips) %>%
  slice_max(votes) %>%
  ungroup()

x_mx <- county_win_data %>% 
  select(-c('county':'pct')) %>% 
  scale(center = T, scale = T)

# compute loadings for PC1 and PC2
x_svd <- svd(x_mx)
d_sq <- x_svd$d^2/(nrow(x_mx) - 1)
loadings <- x_svd$v

# Loadings plot
loadings[, 1:10] %>%
  as.data.frame() %>%
  rename(PC1 = V1, PC2 = V2, PC3 = V3, PC4 = V4, PC5 = V5, 
         PC6 = V6, PC7 = V7, PC8 = V8, PC9 = V9, PC10 = V10) %>%
  mutate(variable = colnames(x_mx)) %>%
  gather(key = 'PC', value = 'Loading', 1:10) %>%
  arrange(variable) %>%
  ggplot(aes(x = Loading, y = variable)) +
  geom_point(aes(color = PC), size = 0.5) +
  facet_wrap(~ PC) +
  theme_bw(base_size = 6) +
  geom_vline(xintercept = 0, color = 'blue') +
  geom_path(aes(group = PC, color = PC)) +
  labs(y = '')
```


## Scree and Cumulative Variance Plot
Using a scree and cumulative variance plot to observe the correct amount of PC's to use in the regression models.
```{r echo = F, fig.width = 5}
## scree and cumulative variance plots
tibble(PC = 1:min(dim(x_mx)),
       Proportion = d_sq/sum(d_sq),
       Cumulative = cumsum(Proportion)) %>%
  gather(key = 'measure', value = 'Variance Explained', 2:3) %>%
  ggplot(aes(x = PC, y = `Variance Explained`)) +
  geom_point(size = 0.75) +
  geom_path() +
  facet_wrap(~ measure) +
  theme_bw(base_size = 6) +
  scale_x_continuous(breaks = 1:31, labels = as.character(1:31)) +
  geom_hline(yintercept = 0.778,
             color = 'red',
             linetype = 2)
sum(d_sq[1:8])/sum(d_sq)
```
To capture 77.8% of the total variation, we need to use the first 8 PC's.

```{r}
# get PC data for first 8 PCs
v_q <- loadings[, 1:8]
Z <- x_mx %*% v_q

colnames(v_q) <- colnames(Z) <- paste('PC', 1:8, sep = '')
pc_data <- tibble(winner = factor(county_win_data$candidate)) %>%
  bind_cols(as_data_frame(Z))
pc_data$winner <- fct_collapse(pc_data$winner,
                               Don = 'Donald Trump',
                               Hill = 'Hillary Clinton')
```

## Split Data (Train/Test)
```{r}
# set rng
set.seed(20121)

# partition data
county_part <- resample_partition(pc_data, c(test = .3, train = .7))
test <- county_part$test
train <- county_part$train
testdata <- pc_data %>% slice(test$idx)
traindata <- pc_data %>% slice(train$idx)
```

## Logistic Regression
```{r }
# Logistic regression model on training data using first 8 PCs
glm_model <- glm(winner ~ ., family = 'binomial', data = traindata)

# compute estimated probabilities
p_hat_glm <- predict(glm_model, testdata, type = 'response')

# bayes classifier
y_hat_glm <- factor(p_hat_glm > 0.5, labels = c('Don', 'Hill'))

# errors
error_glm <- table(y = testdata$winner, y_hat_glm)
error_glm

# total misclassification rate
tot_misclass_glm <- 1 - sum(diag(error_glm))/nrow(testdata)
tot_misclass_glm
```


## K Nearest Neighbors
```{r}
y <- (traindata %>% pull(winner))

# leave one out cross validation
cv_out <- tibble(k = seq_range(5:50, n = 20, pretty = T)) %>%
  mutate(loocv_preds = map(k, ~ knn.cv(traindata[-1], y, .x)),
         class = map(k, ~ y)) %>%
  mutate(misclass = map2(loocv_preds, class, 
                         ~ as.numeric(.x) - as.numeric(.y))) %>%
  mutate(error = map(misclass, ~ mean(abs(.x))))

# error rates for each k
cv_errors <- cv_out %>% 
  select(k, error) %>% 
  unnest(everything()) 

# plot errors against k
cv_errors %>%
  ggplot() +
  geom_line(aes(x = k, y = error), color = 'cornflowerblue') +
  theme_bw()

# select k
best_k <- cv_errors$k[which.min(cv_errors$error)]

# re-train with best k
y_hat_knn <- knn(train = traindata[-1], test = testdata[-1], cl = y, k = best_k)

# misclassifications
error_knn <- table(testdata$winner, y_hat_knn)
error_knn

# total misclassification rate
tot_misclass_knn <- 1 - sum(diag(error_knn))/nrow(testdata)
tot_misclass_knn
```

So far KNN performed better than logistic regression, based on the total misclassification error rate metric.


## Regression Tree
```{r}
#initial tree
nmin <- 5
tree_opts <- tree.control(nobs = nrow(pc_data), 
                          minsize = nmin, 
                          mindev = 0)
t_0 <- tree(winner ~ ., data = pc_data,
                control = tree_opts, split = 'deviance') 

nfolds <- 25
cv_out <- cv.tree(t_0, K = nfolds)
cv_df <- tibble(alpha = cv_out$k,
                impurity = cv_out$dev,
                size = cv_out$size)
best_alpha <- slice_min(cv_df, impurity) %>%
  slice_min(size)

# prune initial tree
t_opt <- prune.tree(t_0, k = best_alpha$alpha)

# plot
draw.tree(t_opt, cex = 0.8, size = 2, digits = 1)

# class probabilities on test partition
probs <- predict(t_opt, pc_data)

# predicted class labels
preds <- factor(probs[, 2] > 0.5, labels = c('Don', 'Hill'))

# misclassification error rates
error_tree <- table(pred = preds, class = pc_data$winner)
misclass_tree = 1-sum(diag(error_tree))/sum(error_tree)
misclass_tree
```


## Random Forest 
```{R}
pc_data_rf <- pc_data %>%
  mutate(winner = ifelse(winner == "Hill",0,1))
traindata <- traindata %>%
      mutate(winner = ifelse(winner == "Hill",0,1))
testdata <- testdata %>%
      mutate(winner = ifelse(winner == "Hill",0,1))



fit_gbm <- gbm(winner ~ ., data = pc_data_rf, distribution ='adaboost', interaction.depth =3, n.trees =100, cv.folds = 5)

# select boosting iterations
best_m <- gbm.perf(fit_gbm, method ='cv')

# compute predictions on test set
preds <- predict(fit_gbm, pc_data_rf, n.trees = best_m)
probs <-1/(1+ exp(-preds))
y_hat <- factor(probs >0.5, labels = c('Don','Hill'))
y <- factor(pc_data_rf$winner, labels = c('Don','Hill'))

# compute misclassification errors
rf_errors <- table(class = y, pred = y_hat)
misclass_rf = 1-sum(diag(rf_errors))/nrow(pc_data)
misclass_rf
```
Random forest shows lowest total misclass.
