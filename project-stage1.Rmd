---
title: "Course project: stage 1"
author: "PSTAT131-231"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      fig.align = 'center',
                      fig.height = 4, 
                      fig.width = 4)

library(pander)
library(tidyverse)
library(ggmap)
library(maps)
setwd("~/Documents/Spring2021/PSTAT131/Project")
```
### Project overview and expectations

Your final project will be to merge census data with 2016 voting data to analyze the election outcome. The work will be carried out in two stages:

1. Preparation and planning (guided)
  + Background reading
  + Data preparation
  + Exploratory analysis
  + Tentative plan for statistical modeling
  
2. Data analysis and reporting (open-ended)
  + Statistical modeling
  + Interpretation of results
  + Report findings
  
This document pertains to the first stage: you'll gather background, preprocess and explore the data, and come up with a tentative plan for the second stage. 

Your objective is to work through the steps outlined in this document, which walk you through data preparation and exploration. The structure is similar to a homework assignment, and your deliverable will be a knitted PDF with all steps filled in.

**Formatting guidelines**

* Your knitted document should not include codes.
* All R output should be nicely formatted; plots should be appropriately labeled and sized, and tables should be passed through `pander()`. Raw R output should not be included.
* Avoid displaying extra plots and figures if they don't show information essential to addressing questions.

**Suggestions for teamwork**

* Set a communication plan -- how will you share your work and when/how will you meet?
* Assign roles -- designate a group member to coordinate communication and another group member to coordinate preparation and submission of your deliverables.
* Divide the work! Discuss your skills and interests and assign each group member specific tasks. Many of the tasks can be carried out in parallel. For those that can't, if some of your group members have more immediate availability, have them work on earlier parts, and have other members follow up on their work by completing later parts. 

**Other comments**

* The plan that you lay out at the end of this document is not a firm committment -- you can always shift directions as you get farther along in the project.
* Negative results are okay. Sometimes an analysis doesn't pan out; predictions aren't good, or inference doesn't identify any significant associations or interesting patterns. Please don't feel that the tasks you propose in this first stage need to generate insights; their merit will be assessed not on their outcome but on whether they aim at thoughtful and interesting questions with a reasonable approach.

**Evaluations**

Our main objective at this stage is to position you well to move forward with an analysis of your choosing, and to provide feedback on your proposal. We may suggest course corrections if we spot anything that we anticipate may pose significant challenges downstream, or encourage you to focus in a particular direction when you start your analysis. Our goal is *not* to judge or criticize your ideas, but rather to help make your project a more rewarding experience. Most credit will be tied to simply completing the guided portions. Here are the basic criteria.

* We'll look for the following in the guided portions (Part 0 -- Part 2):
  + Has your group completed each step successfully?
  + Does your document adhere to the formatting guidelines above?
* We'll look for the following in your proposed tasks:
  + Is the task relevant to understanding or predicting the election outcome?
  + Is a clear plan identified for how to prepare the data for statistical modeling that is appropriate for the task?
  + Is the modeling approach sensible given the task?

# Part 0. Background

The U.S. presidential election in 2012 did not come as a surprise. Some correctly predicted the outcome of the election correctly including [Nate Silver](https://en.wikipedia.org/wiki/Nate_Silver), and [many speculated about his approach](https://www.theguardian.com/science/grrlscientist/2012/nov/08/nate-sliver-predict-us-election).

Despite the success in 2012, the 2016 presidential election came as a [big surprise](https://fivethirtyeight.com/features/the-polls-missed-trump-we-asked-pollsters-why/) to many, and it underscored that predicting voter behavior is complicated for many reasons despite the tremendous effort in collecting, analyzing, and understanding many available datasets.

To familiarize yourself with the general problem of predicting election outcomes, read the articles linked above and answer the following questions. Limit your responses to one short paragraph (3-5 sentences) each.

### Question 0 (a)
What makes voter behavior prediction (and thus election forecasting) a hard problem?

Although we are interested in how people will vote on election day, the data we get is based on how people think they will vote at the time they are asked, that
changes over time. This change might be because of something measurable, such as a change in employment, or if the president doubles federal income tax or can be changes that we can't measure, such as an impactful campaign ad. Another source of variation is sampling error where voters of one 
candidate is overrepresented in the sample, this error can be estimated to some degree and adjusted for. Also if polls are conducted over phone or email there is the case of non-response and/or lying about their voting preference, the corrections by pollsters may be biased due to "house effect".

### Question 0 (b)
What was unique to Nate Silver's approach in 2012 that allowed him to achieve good predictions?

The mathematical model for the proportion of people saying they will vote for a candidate is the actual percentage + "house effect" + sampling variation.

### Question 0 (c)
What went wrong in 2016? What do you think should be done to make future predictions better?

Polls and forecasts were generally wrong about the election due to systematic polling challenges.  Clinton's projected voteshare was overestimated in most cases, particularly within swing states. Experts have speculated that Trump supporters were reluctant to participate and/or answer honestly to polling questions. In the future, I think pollsters should rely less on traditional landline calls and incorporate a wider variety of information-gathering methods, such as cellphone calls, emails, texts, internet polls, and social media web-scraping.

\newpage
# Part 1. Datasets

The `project_data.RData` binary file contains three datasets: tract-level 2010 census data, stored as `census`; metadata `census_meta` with variable descriptions and types; and county-level vote tallies from the 2016 election, stored as `election_raw`.
```{r}
load('data/project_data.RData')
```

## Election data

Some example rows of the election data are shown below:
```{r}
filter(election_raw, !is.na(county)) %>% 
  head() %>% 
  pander()
```

The meaning of each column in `election_raw` is self-evident except `fips`. The accronym is short for [Federal Information Processing Standard](https://en.wikipedia.org/wiki/FIPS_county_code). In this dataset, `fips` values denote the area (nationwide, statewide, or countywide) that each row of data represent.

Nationwide and statewide tallies are included as rows in `election_raw` with `county` values of `NA`. There are two kinds of these summary rows:

* Federal-level summary rows have a `fips` value of `US`.
* State-level summary rows have the state name as the `fips` value.

### Question 1 (a)
Inspect rows with `fips == 2000`. Provide a reason for excluding them. 
```{r, include = F}
# scratch work here for inspection -- will not be included in report
election_raw %>%
  filter(fips == 2000)
election_raw %>%
  filter(is.na(county))
```

Alaska only has one county, so the fips county data is a duplicate of the statewide data.  

### Question 1 (b)
Drop these observations -- please write over `election_raw` -- and report the data dimensions after removal. 
```{r}
# filter out fips == 2000
election_raw <- election_raw %>%
  filter(fips != 2000)
# print dimensions
dim(election_raw)
```

## Census data

The first few rows and columns of the `census` data are shown below.
```{r}
census %>% 
  select(1:6) %>% 
  head() %>% 
  pander(digits = 15)
```
Variable descriptions are given in the `metadata` file. The variables shown above are:
```{r}
census_meta %>% head() %>% pander()
```

## Data preprocessing

### Election data

Currently, the election dataframe is a concatenation of observations (rows) on three kinds of observational units: the country (one observation per candidate); the states (fifty-ish observations per candidate); and counties (most observations in the data frame). These are distinguished by the data type of the `fips` value; for the country observations, `fips == US`; for the state observations, `fips` is a character string (the state name); and for the county observations, `fips` is numeric. In general, it's good practice to format data so that each data table contains observations on only one kind of observational unit.

### Question 1 (c)
Separate `election_raw` into separate federal-, state-, and county-level dataframes:

* Store federal-level tallies as `election_federal`.
    
* Store state-level tallies as `election_state`.
    
* Store county-level tallies as `election`. Coerce the `fips` variable to numeric.

```{r}
# create one dataframe per observational unit
election_raw %>%
  group_by(fips)

election_federal <- election_raw %>%
  filter(fips == 'US')

election_state <- election_raw %>%
  filter(is.na(county), fips != 'US', fips != 46102)

election <- election_raw %>%
  filter(county != 'NA') %>%
  mutate(fips = as.numeric(fips))
```

#### (i) Print the first three rows of `election_federal`. 
Format the table nicely using `pander()`.
```{r}
# print first few rows
election_federal %>%
  head(3) %>% 
  pander()
```

#### (ii) Print the first three rows of `election_state`.
Format the table nicely using `pander()`.
```{r}
# print first few rows
election_state %>% 
  head(3) %>% 
  pander()
```

#### (iii) Print the first three rows of `election`. 
Format the table nicely using `pander()`.
```{r}
# print first few rows
election %>%
  head(3) %>% 
  pander()
```


### Census data

The `census` data contains high resolution information (more fine-grained than county-level). In order to align this with the election data, you'll need to aggregate to the county level, which is the highest geographical resolution available in the election data. The following steps will walk you through this process.

### Question 1 (d)
This first set of initial steps aims to clean up the census data and remove variables that are highly correlated. Write a chain of commands to accomplish the following:

  + filter out any rows of `census` with missing values;

  + convert `Men`, `Women`, `Employed`, and `Citizen` to percentages of the total population;

  + drop `Men`, since the percentage of men is redundant (percent men + percent women = 100)

  + compute a `Minority` variable by summing `Hispanic`, `Black`, `Native`, `Asian`, `Pacific` and then remove these variables after creating `Minority`;

  + remove `Income`, `Walk`, `PublicWork`, and `Construction`; and

  + remove variables whose names end with `Err` (standard errors for estimated quantities).
   
Store the result as `census_clean`, and print the first 3 rows and 7 columns. Format the printed rows and columns nicely using `pander()`.

```{r}
# clean census data
census_clean <- census %>%
  na.omit() %>% 
  mutate(Men = (Men/TotalPop)*100) %>%
  mutate(Women = (Women/TotalPop)*100) %>%
  mutate(Employed = (Employed/TotalPop)*100) %>%
  mutate(Citizen = (Citizen/TotalPop)*100) %>% # these are in decimal form
  mutate(Minority = Hispanic + Black + Native + Asian + Pacific) %>% # Minority is at the end maybe move?
  select(-c(Men,Hispanic, Black, Native, Asian, Pacific, Income, Walk, PublicWork, Construction)) %>%
  select(-(contains('Err')))

# print first few rows/columns
census_clean %>% 
  select(1:7) %>%
  head(3) %>% 
  pander()

```
 
### Question 1 (e) 
To aggregate the clean census data to the county level, you'll weight the variables by population. Create population weights for sub-county census data by following these steps: 

  + group `census_clean` by `State` and `County`;
  
  + use `add_tally()` to add a `CountyPop` variable with the population; 
  
  + add a population weight variable `pop_wt` computed as `TotalPop/CountyPop` (the proportion of the county population in each census tract);
  
  + multiply all quantitative variables by the population weights (use `mutate(across(..., ~ .x*pop_wt));
  
  + remove the grouping structure (`ungroup()`) and drop the population weights and population variables.

Store the result as `census_clean_weighted`, and print the first 3 rows and 7 columns. Format the output nicely using `pander()`.
```{r}
# compute population-weighted quantitative variables
census_clean_weighted <- census_clean %>%
  group_by(State, County) %>%
  add_tally(TotalPop) %>%
  rename(CountyPop = n) %>%
  mutate(pop_wt = TotalPop/CountyPop) %>%
  mutate(across(where(is.numeric),~ .x*pop_wt)) %>%
  ungroup() %>%
  select(-c(pop_wt, CensusTract, TotalPop, CountyPop)) # what other population variables to drop

# print first few rows/columns
census_clean_weighted %>% 
  select(1:7) %>%
  head(3) %>% 
  pander()
```


### Question 1 (f)
Here you'll aggregate the census data to county level. Follow these steps:

  + group the sub-county data `census_clean_weighted` by state and county;
  
  + compute popluation-weighted averages of each variable by taking the sum of each quantitative variable (use `mutate(across(..., sum))`);
  
  + remove the grouping structure.
    
Store the result as `census_tidy` and print the first 3 rows and 7 columns. Format the output nicely using `pander()`.
```{r}
census_tidy <- census_clean_weighted %>%
  group_by(State, County) %>%
  mutate(across(where(is.numeric), sum)) %>%
  ungroup() %>%
  distinct()
  
# print first few rows/columns
census_tidy %>% 
  select(1:7) %>%
  head(3) %>% 
  pander()
names(census_tidy)
```

You can check your final result by comparison with the reference dataset in the .Rmd file for this document containing the first 20 rows of the tidy data.
```{r}
load('data/census-tidy-ref.RData')
census_ref
```


### Question 1 (g)
Now that you have tidy versions of the census and election data, and a merged dataset, clear the raw and intermediate dataframes from your environment using `rm(list = setdiff(ls(), ...))`. `ls()` shows all objects in your environment, so the command removes the set difference between all objects and ones that you specify in place of `...`; the latter should be a vector of the object names you want to keep. You should keep the three data frames containing election data for the federal, state, and county levels, and the tidy census data.
```{r}
# clean up environment
keep <- c('census_tidy', 'election_federal', 'election_state', 'election')
rm(list = setdiff(ls(), keep))
election %>% filter(county == "Barbour County")

```

\newpage
# Part 2: Exploratory analysis
### Question 2 (a)
How many named presidential candidates were there in the 2016 election? Draw a bar graph of all votes received by each candidate, and order the candidate names by decreasing vote counts. (*Hints*: use the federal-level election data; you may need to log-transform the vote axis to see all the bar heights clearly.)
```{r fig.height = 4.5, fig.width = 6.5}
# plotting codes here
election %>%
  dplyr::select(candidate, votes) %>%
  group_by(candidate) %>%
  summarize(total_votes = sum(votes)) %>%
  ggplot(aes(x = total_votes, 
             y = fct_reorder(.f = candidate, .x = total_votes))) +
  geom_bar(stat = 'identity',
           fill = 'lightskyblue') + 
  scale_x_continuous(trans='log2') +
  xlab('Total Votes') +
  ylab('Candidate') +
  labs(title = 'Total Votes Recieved by Each Presidential Candidate in the 2016 Election') +
  theme_bw(base_size = 9)
```


Next you'll generate maps of the election data using `ggmap`. The .Rmd file for this document contains codes to generate a map of the election winner by state. The codes retrieve state geographical boundaries and merge the geographic data with the statewide winner found from the election data by state.
```{r, eval = F}
# plotting boundaries for US states
states <- map_data("state")
name2abb <- function(statename){
  ix <- match(statename, tolower(state.name))
  out <- state.abb[ix]
  return(out)
}
states <- states %>% 
  mutate(fips = name2abb(region))

# who won each state?
state_winner <- election_state %>% # this line depends on your results above!
  group_by(fips) %>% 
  mutate(total = sum(votes), 
         pct = votes/total) %>% 
  slice_max(pct)

# merge winner with plotting boundaries and make map
left_join(states, state_winner) %>%
  ggplot() + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = candidate, 
                   group = group), 
               color = "white", 
               size=0.3) +
  coord_fixed(1.3) +
  guides(fill=FALSE) +
  scale_fill_brewer(palette="Set1") +
  theme_nothing()
```

### Question 2 (b) 
Follow the example above to create a map of the election winner by county. The .Rmd file for this document contains codes to get you started.
```{r message = FALSE}
# plotting boundaries for US counties
counties <- map_data("county")
county_fips <- function(state, countyname){
  ix <- match(paste(state, ',', countyname, sep = ''), county.fips$polyname)
  out <- county.fips$fips[ix]
  return(out)
}
counties <- counties %>%
  mutate(fips = county_fips(region, subregion))

# who won each county?
county_winner <- election %>%
  group_by(fips) %>% 
  mutate(total = sum(votes), 
         pct = votes/total) %>% 
  slice_max(pct)

# merge winner with plotting boundaries and make map
left_join(counties, county_winner) %>%
  ggplot() + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = candidate, 
                   group = group), 
               color = "white", 
               size=0.3) +
  coord_fixed(1.3) +
  guides(fill=FALSE) +
  scale_fill_brewer(palette="Set1") +
  theme_nothing()
```
### Question 2 (c)
Which variables drive variation among counties? Carry out PCA for the census data. 

#### (i) Center and scale the data, compute and plot the principal component loadings for the first two PC's.
```{r, fig.width = 7}
# center and scale
x_mx <- census_tidy %>% 
  select(-c('State', 'County')) %>% 
  scale(center = T, scale = T)
census_tidy
# compute loadings for PC1 and PC2
x_svd <- svd(x_mx)
loadings <- x_svd$v

# plot
loadings[, 1:2] %>%
  as.data.frame() %>%
  rename(PC1 = V1, PC2 = V2) %>%
  mutate(variable = colnames(x_mx)) %>%
  gather(key = 'PC', value = 'Loading', 1:2) %>%
  arrange(variable) %>%
  ggplot(aes(x = variable, y = Loading)) +
  geom_point(aes(shape = PC)) +
  theme_bw() +
  geom_hline(yintercept = 0, color = 'blue') +
  geom_path(aes(linetype = PC, group = PC)) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(x = '')
```

#### (ii) Interpret the loading plot. Which variables drive the variation in the data?  

```{r}
loadings[,1:2] %>% 
  as.data.frame() %>%
  mutate(loading_sum = abs(V1+V2)) %>%
  select(loading_sum) %>%
  mutate(var = colnames(x_mx)) %>%
  arrange(desc(loading_sum)) %>%
  head(n = 7)
```
 
The variables with the highest loading magnitudes drive variation in the data. These variables include 'White' and 'Minority' population rates, poverty rate, child poverty rate, employment and unemployment rates, and percent of workforce in private industry. 

General information about income and employment, such as Child Poverty, Income Per Capita, Employment and Unemployment, corresponds to PC1. Specific professional information, such as Commuter Rate, Work at Home Rate, and Private Work Rate corresponds to PC2. 

#### (iii) How much total variation is captured by the first two principal components?  

```{r, eval = F}
# scratch work here -- don't show codes or output
d_sq <- x_svd$d^2/(nrow(x_mx) - 1)

tibble(pc_var = d_sq,
       pc = paste('PC', 1:ncol(x_mx), sep = '')) %>%
  ggplot(aes(x = fct_reorder(pc, desc(pc_var)), 
             y = cumsum(pc_var)/sum(pc_var))) +
  geom_point() + 
  geom_path() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, size = 6)) +
  labs(x = "PC", y = 'Cumulative variance explained')

sum(d_sq[1:2])/sum(d_sq)
```

The first two PC's capture 41.44% of the total variation.  


#### (iv) Plot PC1 against PC2. 
```{r}
# plotting codes here
census_pc <- as.data.frame(x_mx %*% loadings[,1:2])
colnames(census_pc) <- paste('PC', 1:2, sep = '')

census_pc <- census_pc %>% mutate(County = census_tidy$County)

census_pc %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(size = 0.4, alpha = 0.6) +
  geom_text(aes(label = ifelse((PC2<(-6)&PC1<(-5)), County, ''), hjust = 0, vjust = 0))
```

#### (v) Do you notice any outlier counties? If so, which counties, and why do you think they are outliers?
```{r}
# scratch work here

```

The Kusilvak Census Area is a clear outlier. It has a significantly lower PC2 value than any other county. Oglala Lakota and Todd Counties also have exceptionally low combinations of PC1 and PC2 loading values. Each of these counties have majority Native American populations and exceptionally low income per capita. In fact, they are the three poorest counties in the 50 states according to public data. 

### Question 2 (d)
Create a visualization of your choice using `census` data. Many exit polls noted that [demographics played a big role in the election](https://fivethirtyeight.com/features/demographics-not-hacking-explain-the-election-results/). If you need a starting point, use [this Washington Post article](https://www.washingtonpost.com/graphics/politics/2016-election/exit-polls/) and [this R graph gallery](https://www.r-graph-gallery.com/) for ideas and inspiration.

```{r, fig.width = 8}
abb2name <- function(stateabb){
  ix <- match(stateabb, state.abb)
  out <- tolower(state.name[ix])
  return(out)
}

# top two candidates by county
toptwo <- election %>% 
  group_by(fips) %>% 
  mutate(total = sum(votes), 
         pct = votes/total) %>% 
  slice_max(pct, n = 2)

# create temporary dataframes with matching state/county information
tmpelection <- toptwo %>%
  ungroup %>%
  # coerce names to abbreviations
  mutate(state = abb2name(state)) %>%
  # everything lower case
  mutate(across(c(state, county), tolower)) %>%
  # remove county suffixes
  mutate(county = gsub(" county| columbia| city| parish", 
                       "", 
                       county)) 
tmpcensus <- census_tidy %>% 
  # coerce state and county to lowercase
  mutate(across(c(State, County), tolower))

# merge
merged_data <- tmpelection %>%
  left_join(tmpcensus, 
            by = c("state"="State", "county"="County")) %>% 
  na.omit()

merged_data %>%
  group_by(county, state) %>%
  slice_max(votes) %>%
  ggplot(aes(Minority, Poverty)) +
  geom_point(aes(color = candidate), size = 0.7) +
  xlab('% Minority') +
  ylab('% Under the Poverty Level') +
  ggtitle('Election outcomes in counties by minority and poverty percentages')
```

\newpage
# Part 3: Planned work
Now that you've thought about the prediction problem, tidied and explored the census and election data, you should devise a plan for more focused analysis.

Your objective in the second stage of the project is to analyze a merged county-level dataset. The chunk below this paragraph in the .Rmd file for this document combines the vote information for the winning candidate and runner-up in each county with the census data. 
```{r, eval = F}
# define function to coerce state abbreviations to names
abb2name <- function(stateabb){
  ix <- match(stateabb, state.abb)
  out <- tolower(state.name[ix])
  return(out)
}

# top two candidates by county
toptwo <- election %>% 
  group_by(fips) %>% 
  mutate(total = sum(votes), 
         pct = votes/total) %>% 
  slice_max(pct, n = 2)

# create temporary dataframes with matching state/county information
tmpelection <- toptwo %>%
  ungroup %>%
  # coerce names to abbreviations
  mutate(state = abb2name(state)) %>%
  # everything lower case
  mutate(across(c(state, county), tolower)) %>%
  # remove county suffixes
  mutate(county = gsub(" county| columbia| city| parish", 
                       "", 
                       county)) 
tmpcensus <- census_tidy %>% 
  # coerce state and county to lowercase
  mutate(across(c(State, County), tolower))

# merge
merged_data <- tmpelection %>%
  left_join(tmpcensus, 
            by = c("state"="State", "county"="County")) %>% 
  na.omit()
# clear temporary dataframes from environment
rm(list = c('tmpwinner', 'tmpcensus'))

# print first few rows
merged_data[1:4, 1:8] %>% pander()
}

```

There are a number of possibilities for analyzing this data. Here are just a few:

* Prediction
  + Predict the winner of the popular vote
  + Predict the winner of the general election
  + Predict the winner of each county
  + Predict the vote margin by county
  + Predict the vote margin by state

* Inference
  + Model the probability one candidate wins a county and identify significant associations with census variables
  + Model the vote margin and identify/interpret significant associations with census variables
  + Cluster or group counties and model the probability of a win by one candidate or the vote margin separately for each cluster; look for different patterns of association
  + Model the relationship between votes (or win probabilities) separately for each candidate, and contrast the results.

Each would require some slightly different preprocessing of `merged_data` to select the relevant rows and columns for the specified tasks.

### Question 3
Propose an analysis that you'd like to carry out. Be specific: indicate two tasks you'll pursue and for each task indicate the methods you'll use to approach the task. Your methods description should include mention of how you will prepare `merged_data` for modeling, and which model(s) you'll try.

These descriptions don't need to be long, just enough to convey the general idea. Also, these are not final commitments -- you can always change your mind later on if you like.

#### Task 1

**Task**:
Predict election outcomes 

**Methods**:
Logistic regression

#### Task 2

**Task**:


**Methods**:


